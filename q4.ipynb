{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cf01e0-cb33-42fa-a7aa-3b58d93d75ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /opt/conda/envs/bigdata/lib/python3.12/site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /opt/conda/envs/bigdata/lib/python3.12/site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ce073f4-33ae-4dc3-8c18-dc6add198ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/10 22:43:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created.\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, avg, count, min, max, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# 1. Create your Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Q4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37b075f-f2d1-48a3-a81c-e5bb90931f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring directory: midterm/data/q4/\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"midterm/data/q4/\"\n",
    "\n",
    "# schema definition\n",
    "schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True) # Read as string first\n",
    "])\n",
    "\n",
    "# reading the stream from the input directory\n",
    "df_raw = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .json(input_directory)\n",
    "\n",
    "# convert string timestamp to actual timestamp object\n",
    "# adding a 2-minute watermark as was required\n",
    "df_sensors = df_raw.withColumn(\n",
    "    \"timestamp\",\n",
    "    to_timestamp(col(\"timestamp\"))\n",
    ").withWatermark(\"timestamp\", \"2 minutes\")\n",
    "\n",
    "print(f\"Monitoring directory: {input_directory}\")\n",
    "df_sensors.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a506af-5608-43d9-a27d-304880c96248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 22:46:35 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4b07feee-5e15-4975-ab6e-2664c0c9b96e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/10 22:46:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 'q4_part_a_results' started in 'complete' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 22:46:47 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 11982 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# --- Part A: Simple Aggregations ---\n",
    "df_part_A = df_sensors.groupBy(\"sensor_id\").agg(\n",
    "    avg(\"temperature\").alias(\"avg_temp\"),\n",
    "    count(\"*\").alias(\"reading_count\"),\n",
    "    min(\"temperature\").alias(\"min_temp\"),\n",
    "    max(\"temperature\").alias(\"max_temp\")\n",
    ")\n",
    "\n",
    "# OutputMode 'complete': The entire updated result table is written.\n",
    "# Format 'memory': Writes to an in-memory table named 'q4_part_a_results'.\n",
    "query_A = df_part_A.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"q4_part_a_results\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(f\"Stream 'q4_part_a_results' started in 'complete' mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458d761f-6f72-45f8-b112-ec6570815365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 22:46:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6c438b60-fc4d-431b-90a8-ef93944276f8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/10 22:46:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/11/10 22:46:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-36b90823-a6f5-4917-aa11-b7dd69070a98. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/11/10 22:46:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 'q4_part_b_tumbling_results' started in 'append' mode.\n",
      "Stream 'q4_part_b_sliding_results' started in 'append' mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# --- Part B: Windowed Aggregations ---\n",
    "\n",
    "# 1. 5-minute tumbling window: Avg temp and count across ALL sensors [cite: 155, 157]\n",
    "df_part_B_tumbling = df_sensors \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"5 minutes\") # Tumbling window\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"temperature\").alias(\"avg_temp_5min\"),\n",
    "        count(\"*\").alias(\"count_5min\")\n",
    "    ) \\\n",
    "    .select(\"window.start\", \"window.end\", \"avg_temp_5min\", \"count_5min\")\n",
    "\n",
    "# 2. 10-minute sliding window (slide 5 min): Max temp and count PER sensor [cite: 156, 157]\n",
    "df_part_B_sliding = df_sensors \\\n",
    "    .groupBy(\n",
    "        col(\"sensor_id\"),\n",
    "        window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\") # Sliding window\n",
    "    ) \\\n",
    "    .agg(\n",
    "        max(\"temperature\").alias(\"max_temp_10min\"),\n",
    "        count(\"*\").alias(\"count_10min\")\n",
    "    ) \\\n",
    "    .select(\"window.start\", \"window.end\", \"sensor_id\", \"max_temp_10min\", \"count_10min\")\n",
    "\n",
    "# 3. Start the queries for Part B\n",
    "# OutputMode 'append': Only *completed* windows are written.\n",
    "query_B_tumbling = df_part_B_tumbling.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"q4_part_b_tumbling_results\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query_B_sliding = df_part_B_sliding.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"q4_part_b_sliding_results\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream 'q4_part_b_tumbling_results' started in 'append' mode.\")\n",
    "print(\"Stream 'q4_part_b_sliding_results' started in 'append' mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961803cd-0fae-44e0-b9ce-e4b5461ca33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 10 seconds for streams to process...\n",
      "\n",
      "--- Part A Results (Running Aggregations) ---\n",
      "+---------+-----------------+-------------+--------+--------+\n",
      "|sensor_id|avg_temp         |reading_count|min_temp|max_temp|\n",
      "+---------+-----------------+-------------+--------+--------+\n",
      "|S001     |72.5676036400404 |989          |55.16   |86.52   |\n",
      "|S002     |72.29238505747124|1044         |55.79   |88.86   |\n",
      "|S003     |72.65653725078695|953          |56.18   |89.88   |\n",
      "|S004     |72.31032901296105|1003         |56.18   |89.93   |\n",
      "|S005     |72.89840625000005|960          |56.43   |89.96   |\n",
      "|S006     |72.53573964497033|1014         |55.18   |89.75   |\n",
      "|S007     |72.57789686552077|989          |55.17   |88.78   |\n",
      "|S008     |72.28547169811334|1007         |56.5    |89.89   |\n",
      "|S009     |72.79977905859754|1041         |56.16   |89.82   |\n",
      "|S010     |72.53289000000001|1000         |56.0    |89.33   |\n",
      "+---------+-----------------+-------------+--------+--------+\n",
      "\n",
      "\n",
      "--- Part B Results (5-min Tumbling Window) ---\n",
      "+-------------------+-------------------+-----------------+----------+\n",
      "|start              |end                |avg_temp_5min    |count_5min|\n",
      "+-------------------+-------------------+-----------------+----------+\n",
      "|2024-01-15 10:00:00|2024-01-15 10:05:00|72.73390243902439|41        |\n",
      "|2024-01-15 10:05:00|2024-01-15 10:10:00|71.5245          |40        |\n",
      "|2024-01-15 10:10:00|2024-01-15 10:15:00|72.62658536585367|41        |\n",
      "|2024-01-15 10:15:00|2024-01-15 10:20:00|72.87131578947368|38        |\n",
      "|2024-01-15 10:20:00|2024-01-15 10:25:00|72.326           |40        |\n",
      "|2024-01-15 10:25:00|2024-01-15 10:30:00|73.5441463414634 |41        |\n",
      "|2024-01-15 10:30:00|2024-01-15 10:35:00|73.68923076923079|39        |\n",
      "|2024-01-15 10:35:00|2024-01-15 10:40:00|71.73538461538459|39        |\n",
      "|2024-01-15 10:40:00|2024-01-15 10:45:00|73.92300000000002|40        |\n",
      "|2024-01-15 10:45:00|2024-01-15 10:50:00|72.26023255813952|43        |\n",
      "|2024-01-15 10:50:00|2024-01-15 10:55:00|72.36583333333334|36        |\n",
      "|2024-01-15 10:55:00|2024-01-15 11:00:00|72.41642857142858|42        |\n",
      "|2024-01-15 11:00:00|2024-01-15 11:05:00|72.9025          |40        |\n",
      "|2024-01-15 11:05:00|2024-01-15 11:10:00|73.41954545454544|44        |\n",
      "|2024-01-15 11:10:00|2024-01-15 11:15:00|72.0085294117647 |34        |\n",
      "|2024-01-15 11:15:00|2024-01-15 11:20:00|72.11375         |40        |\n",
      "|2024-01-15 11:20:00|2024-01-15 11:25:00|71.96232558139535|43        |\n",
      "|2024-01-15 11:25:00|2024-01-15 11:30:00|73.22634146341463|41        |\n",
      "|2024-01-15 11:30:00|2024-01-15 11:35:00|73.22416666666668|36        |\n",
      "|2024-01-15 11:35:00|2024-01-15 11:40:00|71.93674418604652|43        |\n",
      "+-------------------+-------------------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "--- Part B Results (10-min Sliding Window) ---\n",
      "+-------------------+-------------------+---------+--------------+-----------+\n",
      "|start              |end                |sensor_id|max_temp_10min|count_10min|\n",
      "+-------------------+-------------------+---------+--------------+-----------+\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S001     |78.69         |5          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S002     |79.12         |3          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S003     |76.58         |6          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S004     |77.93         |3          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S005     |81.8          |4          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S006     |77.13         |3          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S007     |71.33         |2          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S008     |77.54         |5          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S009     |79.52         |7          |\n",
      "|2024-01-15 09:55:00|2024-01-15 10:05:00|S010     |73.91         |3          |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S001     |78.69         |9          |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S002     |79.12         |5          |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S003     |76.58         |11         |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S004     |77.93         |11         |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S005     |81.8          |6          |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S006     |77.13         |7          |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S007     |77.31         |7          |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S008     |78.82         |9          |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S009     |79.52         |11         |\n",
      "|2024-01-15 10:00:00|2024-01-15 10:10:00|S010     |76.76         |5          |\n",
      "+-------------------+-------------------+---------+--------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Give the streams a moment to process the file\n",
    "print(\"Waiting 10 seconds for streams to process...\")\n",
    "time.sleep(10) \n",
    "\n",
    "print(\"\\n--- Part A Results (Running Aggregations) ---\")\n",
    "spark.sql(\"SELECT * FROM q4_part_a_results ORDER BY sensor_id\").show(truncate=False)\n",
    "\n",
    "print(\"\\n--- Part B Results (5-min Tumbling Window) ---\")\n",
    "spark.sql(\"SELECT * FROM q4_part_b_tumbling_results ORDER BY start\").show(truncate=False)\n",
    "\n",
    "print(\"\\n--- Part B Results (10-min Sliding Window) ---\")\n",
    "spark.sql(\"SELECT * FROM q4_part_b_sliding_results ORDER BY start, sensor_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a87ebcc-91ad-4481-9c6a-6f49aea0dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all streams...\n",
      "Stopping stream: q4_part_b_tumbling_results\n",
      "Stopping stream: q4_part_b_sliding_results\n",
      "Stopping stream: q4_part_a_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/10 22:47:56 WARN DAGScheduler: Failed to cancel job group c522564e-358a-4815-a335-9eb1a0951e77. Cannot find active jobs for it.\n",
      "25/11/10 22:47:56 WARN DAGScheduler: Failed to cancel job group c522564e-358a-4815-a335-9eb1a0951e77. Cannot find active jobs for it.\n",
      "25/11/10 22:47:56 WARN DAGScheduler: Failed to cancel job group c0793d97-e1fe-457f-9836-5caa290d03b6. Cannot find active jobs for it.\n",
      "25/11/10 22:47:56 WARN DAGScheduler: Failed to cancel job group c0793d97-e1fe-457f-9836-5caa290d03b6. Cannot find active jobs for it.\n",
      "25/11/10 22:47:56 WARN DAGScheduler: Failed to cancel job group cae7a6a2-2e10-4425-a654-ee2dae1ec83b. Cannot find active jobs for it.\n",
      "25/11/10 22:47:56 WARN DAGScheduler: Failed to cancel job group cae7a6a2-2e10-4425-a654-ee2dae1ec83b. Cannot find active jobs for it.\n",
      "25/11/10 22:48:45 WARN StateStore: Error running maintenance thread\n",
      "java.lang.IllegalStateException: SparkEnv not active, cannot do maintenance on StateStores\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:971)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:945)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:746)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "print(\"Stopping all streams...\")\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping stream: {stream.name}\")\n",
    "    try:\n",
    "        stream.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not stop stream {stream.name}: {e}\")\n",
    "\n",
    "# Stopping the Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2512d-ac51-479d-9465-dfbec0794ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
